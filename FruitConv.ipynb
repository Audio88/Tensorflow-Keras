{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 32, 32 \n",
    "\n",
    "# we set the img_width and img_height according to the pretrained models we are\n",
    "# going to use. The input size for ResNet-50 is 224 by 224 by 3.\n",
    "train_data_dir = 'fruits/fruits-360/Training/'\n",
    "validation_data_dir = 'fruits/fruits-360/Test/'\n",
    "nb_train_samples = 36117\n",
    "nb_validation_samples = 12132\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36117 images belonging to 72 classes.\n",
      "Found 12132 images belonging to 72 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2258/2258 [==============================] - 59s 26ms/step - loss: 1.5237 - acc: 0.5627 - val_loss: 0.3371 - val_acc: 0.8924\n",
      "Epoch 2/10\n",
      "2258/2258 [==============================] - 57s 25ms/step - loss: 0.4438 - acc: 0.8528 - val_loss: 0.1567 - val_acc: 0.9527\n",
      "Epoch 3/10\n",
      "2258/2258 [==============================] - 56s 25ms/step - loss: 0.2887 - acc: 0.9028 - val_loss: 0.1405 - val_acc: 0.9615\n",
      "Epoch 4/10\n",
      "2258/2258 [==============================] - 56s 25ms/step - loss: 0.2265 - acc: 0.9233 - val_loss: 0.1401 - val_acc: 0.9488\n",
      "Epoch 5/10\n",
      "2258/2258 [==============================] - 56s 25ms/step - loss: 0.1927 - acc: 0.9332 - val_loss: 0.1202 - val_acc: 0.9604\n",
      "Epoch 6/10\n",
      "2258/2258 [==============================] - 55s 24ms/step - loss: 0.1713 - acc: 0.9413 - val_loss: 0.0940 - val_acc: 0.9721\n",
      "Epoch 7/10\n",
      "2258/2258 [==============================] - 55s 24ms/step - loss: 0.1598 - acc: 0.9466 - val_loss: 0.1055 - val_acc: 0.9659\n",
      "Epoch 8/10\n",
      "2258/2258 [==============================] - 55s 24ms/step - loss: 0.1484 - acc: 0.9503 - val_loss: 0.0956 - val_acc: 0.9698\n",
      "Epoch 9/10\n",
      "2258/2258 [==============================] - 55s 24ms/step - loss: 0.1387 - acc: 0.9538 - val_loss: 0.1172 - val_acc: 0.9709\n",
      "Epoch 10/10\n",
      "2258/2258 [==============================] - 56s 25ms/step - loss: 0.1319 - acc: 0.9565 - val_loss: 0.0868 - val_acc: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a7d6b2cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(72, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "            train_generator,\n",
    "            epochs=10,\n",
    "            verbose = 1,\n",
    "            validation_data = validation_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
